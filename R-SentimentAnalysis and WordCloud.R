install.packages("twitteR", dependencies=T)
install.packages("RCurl")
install.packages("RJSONIO")
install.packages("stringr")
install.packages("tm")
install.packages(c("wordcloud","tm"),repos="http://cran.r-project.org")
install.packages("plyr")

library(twitteR)
library(plyr)
require(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(tm)
library(wordcloud)

#specify consumer and access tokens to access twitter API
consumerKey <- "kSCpBRfaUq5MLghSXJlCapFdO"

consumerSecret <- "5Khk7BmxGFhy2JhuygPltZ1QXsDieR8MAewtfotrWpxaoyXZOZ"

access_token <- "752734613034921984-l2VLK3S6MRBTJWiwMG1aZ0gvSPLYrNn"

access_token_secret <- "Zzq9thg3GGea3O8bKWAMzQ2KbyT77N2PxbByXocNC7mqj"

setup_twitter_oauth(consumerKey, consumerSecret, access_token, access_token_secret)

#to fetch the oscar nominated movies
df <- read.csv('/Users/Manya Mittal/Desktop/Fall 2017/Big Data Technologies/Data.csv')
library(sqldf)
df_movie <- sqldf("Select distinct movie_title from df where oscar_nomination = 'YES';")

#use twitter API to search for tweets related to the oscar nominated movies
tweets <-searchTwitter("The Lord of the Rings", n = 100)
tweets2 <- searchTwitter("Raging Bull", n=100)
tweets3 <- searchTwitter("Winter's Bone", n=100)
tweets4 <- searchTwitter("A Serious Man", n=100)
tweets5 <- searchTwitter("The Hurt Locker", n=100)
tweets6 <- searchTwitter("127 Hours", n=100)
tweets7 <- searchTwitter("Frost/Nixon", n=100)
tweets8 <- searchTwitter("The Kids Are All Right", n=100)
tweets9 <- searchTwitter("The Postman", n=100)
tweets10 <- searchTwitter("The Alamo", n=100)
tweets12 <- searchTwitter("Around the World in 80 Days", n=100)
tweets13 <- searchTwitter("Milk", n=100)
tweets14 <- searchTwitter("The Reader", n=100)
tweets15 <- searchTwitter("Good Night, and Good Luck", n=100)
tweets16 <- searchTwitter("The Insider", n=100)
tweets17 <- searchTwitter("42nd Street", n=100)
tweets18 <- searchTwitter("The Shawshank Redemption", n=100)
tweets19 <- searchTwitter("There Will Be Blood", n=100)
tweets20 <- searchTwitter("In the Bedroom", n=100)
tweets21 <- searchTwitter("The Right Stuff", n=100)
tweets22 <- searchTwitter("Top Hat", n=100)
tweets23 <- searchTwitter("Precious", n=100)
tweets24 <- searchTwitter("The Thin Red Line", n=100)
tweets25 <- searchTwitter("The Hours", n=100)
tweets26 <- searchTwitter("Michael Clayton", n=100)
tweets27 <- searchTwitter("Lost in Translation", n=100)
tweets28 <- searchTwitter("Finding Neverland", n=100)
tweets29 <- searchTwitter("Sense and Sensibility", n=100)
tweets30 <- searchTwitter("Dangerous Liaisons", n=100)
tweets31 <- searchTwitter("Little Miss Sunshine", n=100)
tweets32 <- searchTwitter("Moulin Rouge", n=100)
tweets33 <- searchTwitter("Little Women", n=100)
tweets34 <- searchTwitter("On the Waterfront", n=100)
tweets35 <- searchTwitter("No Country for Old Men", n=100)
tweets36 <- searchTwitter("Sideways", n=100)
tweets37 <- searchTwitter("Up in the Air", n=100)
tweets38 <- searchTwitter("Ray", n=100)
tweets39 <- searchTwitter("L.A. Confidential", n=100)
tweets40 <- searchTwitter("Brokeback Mountain", n=100)
tweets41 <- searchTwitter("The Fighter", n=100)
tweets42 <- searchTwitter("Gangs of New York", n=100)
tweets43 <- searchTwitter("The Social Network", n=100)
tweets44 <- searchTwitter("Mystic River", n=100)
tweets45 <- searchTwitter("Black Swan", n=100)
tweets46 <- searchTwitter("Braveheart", n=100)
tweets47 <- searchTwitter("Amadeus", n=100)
tweets48 <- searchTwitter("The English Patient", n=100)
tweets49 <- searchTwitter("Million Dollar Baby", n=100)
tweets50 <- searchTwitter("The Aviator", n=100)
tweets51 <- searchTwitter("The Prince of Tides", n=100)
tweets52 <- searchTwitter("The Verdict", n=100)
tweets53 <- searchTwitter("Inglourious Basterds", n=100)
tweets54 <- searchTwitter("Born on the Fourth of July", n=100)
tweets55 <- searchTwitter("The Curious Case of Benjamin Button", n=100)
tweets56 <- searchTwitter("Witness", n=100)
tweets57 <- searchTwitter("Shakespeare in Love", n=100)
tweets58 <- searchTwitter("Annie Hall", n=100)
tweets59 <- searchTwitter("Seabiscuit", n=100)
tweets60 <- searchTwitter("The Departed", n=100)
tweets61 <- searchTwitter("Ordinary People", n=100)
tweets62 <- searchTwitter("Schindler's List", n=100)
tweets63 <- searchTwitter("Juno", n=100)
tweets64 <- searchTwitter("Traffic", n=100)
tweets65 <- searchTwitter("Unforgiven", n=100)
tweets66 <- searchTwitter("Pulp Fiction", n=100)
tweets67 <- searchTwitter("Erin Brockovich", n=100)
tweets68 <- searchTwitter("Dead Poets Society", n=100)
tweets69 <- searchTwitter("American Beauty", n=100)
tweets70 <- searchTwitter("True Grit", n=100)
tweets71 <- searchTwitter("Out of Africa", n=100)
tweets72 <- searchTwitter("The Green Mile", n=100)
tweets73 <- searchTwitter("Driving Miss Daisy", n=100)
tweets74 <- searchTwitter("Good Will Hunting", n=100)
tweets75 <- searchTwitter("The Color Purple", n=100)
tweets76 <- searchTwitter("Doctor Dolittle", n=100)
tweets77 <- searchTwitter("As Good as It Gets", n=100)
tweets78 <- searchTwitter("Chicago", n=100)
tweets79 <- searchTwitter("The Silence of the Lambs", n=100)
tweets80 <- searchTwitter("A Beautiful Mind", n=100)
tweets81 <- searchTwitter("Coal Miner's Daughter", n=100)
tweets82 <- searchTwitter("Jerry Maguire", n=100)
tweets83 <- searchTwitter("A Few Good Men", n=100)
tweets84 <- searchTwitter("Apocalypse Now", n=100)
tweets85 <- searchTwitter("Gladiator", n=100)
tweets86 <- searchTwitter("Apollo 13", n=100)
tweets87 <- searchTwitter("The Blind Side", n=100)
tweets88 <- searchTwitter("The Best Years of Our Lives", n=100)
tweets89 <- searchTwitter("Fiddler on the Roof", n=100)
tweets90 <- searchTwitter("The Fugitive", n=100)
tweets91 <- searchTwitter("Saving Private Ryan", n=100)
tweets92 <- searchTwitter("Inception", n=100)
tweets93 <- searchTwitter("The Robe", n=100)
tweets94 <- searchTwitter("The Greatest Show on Earth", n=100)
tweets95 <- searchTwitter("Up", n=100)
tweets96 <- searchTwitter("Fatal Attraction", n=100)
tweets97 <- searchTwitter("Dances with Wolves", n=100)
tweets98 <- searchTwitter("West Side Story", n=100)
tweets99 <- searchTwitter("Rain Man", n=100)
tweets100 <- searchTwitter("The Wizard of Oz", n=100)
tweets101 <- searchTwitter("Ghost", n=100)
tweets102 <- searchTwitter("The Sixth Sense", n=100)
tweets103 <- searchTwitter("Tootsie", n=100)
tweets104 <- searchTwitter("Toy Story 3", n=100)
tweets105 <- searchTwitter("Rocky", n=100)
tweets106 <- searchTwitter("One Flew Over the Cuckoo's Nest", n=100)
tweets107 <- searchTwitter("Forrest Gump", n=100)
tweets108 <- searchTwitter("My Fair Lady", n=100)
tweets109 <- searchTwitter("American Graffiti", n=100)
tweets110 <- searchTwitter("Raiders of the Lost Ark", n=100)
tweets111 <- searchTwitter("Butch Cassidy and the Sundance Kid", n=100)
tweets112 <- searchTwitter("Mary Poppins", n=100)
tweets113 <- searchTwitter("Avatar", n=100)
tweets114 <- searchTwitter("Doctor Zhivago", n=100)
tweets115 <- searchTwitter("The Sting", n=100)
tweets116 <- searchTwitter("Titanic", n=100)
tweets117 <- searchTwitter("E.T. the Extra-Terrestrial", n=100)
tweets118 <- searchTwitter("The Exorcist", n=100)
tweets119 <- searchTwitter("Jaws", n=100)
tweets120 <- searchTwitter("The Sound of Music", n=100)
tweets121 <- searchTwitter("Gone with the Wind", n=100)

#make one dataset with all the searched tweets
tweets_data<-tweets
tweets_data <- append(tweets_data,tweets2)
tweets_data<-append(tweets_data,tweets3)
tweets_data<-append(tweets_data,tweets4)
tweets_data<-append(tweets_data,tweets5)
tweets_data<-append(tweets_data,tweets6)
tweets_data<-append(tweets_data,tweets7)
tweets_data<-append(tweets_data,tweets8)
tweets_data<-append(tweets_data,tweets9)
tweets_data<-append(tweets_data,tweets10)
tweets_data<-append(tweets_data,tweets12)
tweets_data<-append(tweets_data,tweets13)
tweets_data<-append(tweets_data,tweets14)
tweets_data<-append(tweets_data,tweets15)
tweets_data<-append(tweets_data,tweets16)
tweets_data<-append(tweets_data,tweets17)
tweets_data<-append(tweets_data,tweets18)
tweets_data<-append(tweets_data,tweets19)
tweets_data<-append(tweets_data,tweets20)
tweets_data<-append(tweets_data,tweets21)
tweets_data<-append(tweets_data,tweets22)
tweets_data<-append(tweets_data,tweets23)
tweets_data<-append(tweets_data,tweets24)
tweets_data<-append(tweets_data,tweets25)
tweets_data<-append(tweets_data,tweets26)
tweets_data<-append(tweets_data,tweets27)
tweets_data<-append(tweets_data,tweets28)
tweets_data<-append(tweets_data,tweets29)
tweets_data<-append(tweets_data,tweets30)
tweets_data<-append(tweets_data,tweets31)
tweets_data<-append(tweets_data,tweets32)
tweets_data<-append(tweets_data,tweets33)
tweets_data<-append(tweets_data,tweets34)
tweets_data<-append(tweets_data,tweets35)
tweets_data<-append(tweets_data,tweets36)
tweets_data<-append(tweets_data,tweets37)
tweets_data<-append(tweets_data,tweets38)
tweets_data<-append(tweets_data,tweets39)
tweets_data<-append(tweets_data,tweets40)
tweets_data<-append(tweets_data,tweets41)
tweets_data<-append(tweets_data,tweets42)
tweets_data<-append(tweets_data,tweets43)
tweets_data<-append(tweets_data,tweets44)
tweets_data<-append(tweets_data,tweets45)
tweets_data<-append(tweets_data,tweets46)
tweets_data<-append(tweets_data,tweets47)
tweets_data<-append(tweets_data,tweets48)
tweets_data<-append(tweets_data,tweets49)
tweets_data<-append(tweets_data,tweets50)
tweets_data<-append(tweets_data,tweets51)
tweets_data<-append(tweets_data,tweets52)
tweets_data<-append(tweets_data,tweets53)
tweets_data<-append(tweets_data,tweets54)
tweets_data<-append(tweets_data,tweets55)
tweets_data<-append(tweets_data,tweets56)
tweets_data<-append(tweets_data,tweets57)
tweets_data<-append(tweets_data,tweets58)
tweets_data<-append(tweets_data,tweets59)
tweets_data<-append(tweets_data,tweets60)
tweets_data<-append(tweets_data,tweets61)
tweets_data<-append(tweets_data,tweets62)
tweets_data<-append(tweets_data,tweets63)
tweets_data<-append(tweets_data,tweets64)
tweets_data<-append(tweets_data,tweets65)
tweets_data<-append(tweets_data,tweets66)
tweets_data<-append(tweets_data,tweets67)
tweets_data<-append(tweets_data,tweets68)
tweets_data<-append(tweets_data,tweets69)
tweets_data<-append(tweets_data,tweets70)
tweets_data<-append(tweets_data,tweets71)
tweets_data<-append(tweets_data,tweets72)
tweets_data<-append(tweets_data,tweets73)
tweets_data<-append(tweets_data,tweets74)
tweets_data<-append(tweets_data,tweets75)
tweets_data<-append(tweets_data,tweets76)
tweets_data<-append(tweets_data,tweets77)
tweets_data<-append(tweets_data,tweets78)
tweets_data<-append(tweets_data,tweets79)
tweets_data<-append(tweets_data,tweets80)
tweets_data<-append(tweets_data,tweets81)
tweets_data<-append(tweets_data,tweets82)
tweets_data<-append(tweets_data,tweets83)
tweets_data<-append(tweets_data,tweets84)
tweets_data<-append(tweets_data,tweets85)
tweets_data<-append(tweets_data,tweets86)
tweets_data<-append(tweets_data,tweets87)
tweets_data<-append(tweets_data,tweets88)
tweets_data<-append(tweets_data,tweets89)
tweets_data<-append(tweets_data,tweets90)
tweets_data<-append(tweets_data,tweets91)
tweets_data<-append(tweets_data,tweets92)
tweets_data<-append(tweets_data,tweets93)
tweets_data<-append(tweets_data,tweets94)
tweets_data<-append(tweets_data,tweets95)
tweets_data<-append(tweets_data,tweets96)
tweets_data<-append(tweets_data,tweets97)
tweets_data<-append(tweets_data,tweets98)
tweets_data<-append(tweets_data,tweets99)
tweets_data<-append(tweets_data,tweets100)
tweets_data<-append(tweets_data,tweets101)
tweets_data<-append(tweets_data,tweets102)
tweets_data<-append(tweets_data,tweets103)
tweets_data<-append(tweets_data,tweets104)
tweets_data<-append(tweets_data,tweets105)
tweets_data<-append(tweets_data,tweets106)
tweets_data<-append(tweets_data,tweets107)
tweets_data<-append(tweets_data,tweets108)
tweets_data<-append(tweets_data,tweets109)
tweets_data<-append(tweets_data,tweets110)
tweets_data<-append(tweets_data,tweets111)
tweets_data<-append(tweets_data,tweets112)
tweets_data<-append(tweets_data,tweets113)
tweets_data<-append(tweets_data,tweets114)
tweets_data<-append(tweets_data,tweets115)
tweets_data<-append(tweets_data,tweets116)
tweets_data<-append(tweets_data,tweets117)
tweets_data<-append(tweets_data,tweets118)
tweets_data<-append(tweets_data,tweets119)
tweets_data<-append(tweets_data,tweets120)
tweets_data<-append(tweets_data,tweets121)

###################Sentiment Analysis#####################

#use lapply function fetch the text data
Tweets.text = laply(tweets_data,function(t)t$getText())

#convert data into a usable form
usableText=str_replace_all(Tweets.text,"[^[:graph:]]", " ") 

#store a list of positive and negative words
pos = scan('/Users/Manya Mittal/Desktop/Fall 2017/Big Data Technologies/TwitterMining/positive.txt', what='character', comment.char=';')
neg = scan('/Users/Manya Mittal/Desktop/Fall 2017/Big Data Technologies/TwitterMining/negative.txt', what='character', comment.char=';')

#assign a sentiment score to the positive and negative words
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)
  require(stringr)
  # we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
  # we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub("@\\w+ *", "", sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    # compare our words to the dictionaries of positive & negative terms
    neg.matches = match(words, neg.words)
    pos.matches = match(words, pos.words)
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}

#run analysis on the scores (use function)
analysis = score.sentiment(usableText, pos, neg)
table(analysis$score)

#plot scores along with their frequency
hist(analysis$score, main = "Sentiment Score", xlab ="Score", ylab = "Frequency")

#######################Word Cloud######################

#use sapply function to fetch text
tweet_txt = sapply(tweets_data, function(x) x$getText())

#convert data into a usable form
usableText2=str_replace_all(tweet_txt,"[^[:graph:]]", " ")
usableText2=str_replace_all(tweet_txt,"https","")

#function to clean the text field
clean.text <- function(some_txt)
{
  some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
  some_txt = gsub("@\\w+", "", some_txt)
  some_txt = gsub("[[:punct:]]", "", some_txt)
  some_txt = gsub("[[:digit:]]", "", some_txt)
  some_txt = gsub("http\\w+", "", some_txt)
  some_txt = gsub("[ \t]{2,}", "", some_txt)
  some_txt = gsub("^\\s+|\\s+$", "", some_txt)
  some_txt = gsub("amp", "", some_txt)
  # define "tolower error handling" function
  try.tolower = function(x)
  {
    y = NA
    try_error = tryCatch(tolower(x), error=function(e) e)
    if (!inherits(try_error, "error"))
      y = tolower(x)
    return(y)
  }
  some_txt = sapply(some_txt, try.tolower)
  some_txt = some_txt[some_txt != ""]
  names(some_txt) = NULL
  return(some_txt)
}

#apply function on usable form data
clean_text = clean.text(usableText2)

clean_text = removeWords(clean_text,stopwords("english"))

tweet_corpus = Corpus(VectorSource(usableText2))
tdm = TermDocumentMatrix(tweet_corpus, control = list(removePunctuation = TRUE,stopwords = c("machine", "learning", stopwords("english")), removeNumbers = TRUE, tolower = TRUE))


#defining tdm as matrix
m = as.matrix(tdm)

#get the word orders in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE) 

#create a dataset
dm = data.frame(word=names(word_freqs), freq=word_freqs)

#visualize the wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))

